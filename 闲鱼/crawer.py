import os
import re
import json
import csv
import logging
from datetime import datetime
from typing import Dict, Any, List
from dotenv import load_dotenv
from playwright.sync_api import Playwright, sync_playwright

load_dotenv()
def setup_logging():
    class ColoredFormatter(logging.Formatter):
        COLORS = {
            'DEBUG': '\033[36m',    
            'INFO': '\033[32m',     
            'WARNING': '\033[33m',  
            'ERROR': '\033[31m',    
            'CRITICAL': '\033[35m', 
            'RESET': '\033[0m'      
        }
        def format(self, record):
            color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
            reset = self.COLORS['RESET']
            record.asctime = self.formatTime(record, '%H:%M:%S')
            if record.levelname == 'INFO':
                log_format = f"{color}[%(asctime)s] ğŸ“‹ %(message)s{reset}"
            elif record.levelname == 'WARNING':
                log_format = f"{color}[%(asctime)s] âš ï¸  %(message)s{reset}"
            elif record.levelname == 'ERROR':
                log_format = f"{color}[%(asctime)s] âŒ %(message)s{reset}"
            elif record.levelname == 'DEBUG':
                log_format = f"{color}[%(asctime)s] ğŸ” %(message)s{reset}"
            else:
                log_format = f"{color}[%(asctime)s] %(levelname)s: %(message)s{reset}"
            formatter = logging.Formatter(log_format)
            return formatter.format(record)
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(ColoredFormatter())
    logger.addHandler(console_handler)
    return logger
logger = setup_logging()
def log_step(step_num, title):
    separator = "=" * 50
    logger.info(f"\n{separator}")
    logger.info(f"ğŸ”„ æ­¥éª¤ {step_num}: {title}")
    logger.info(separator)
all_products_data = []
item_details_cache = {}
def convert_timestamp(timestamp: str) -> str:
    if not timestamp:
        return ""
    try:
        dt = datetime.fromtimestamp(int(timestamp) / 1000)
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, TypeError):
        return timestamp
def extract_item_info(item_data: Dict[str, Any]) -> Dict[str, Any]:
    if "main" not in item_data:
        return None
    main = item_data["main"]
    click_param = main.get("clickParam", {})
    args = click_param.get("args", {})
    ex_content = main.get("exContent", {})
    item_info = {
        "åºå·": "",
        "å•†å“ID": args.get("id", ""),
        "å•†å“æ ‡é¢˜": "",
        "ä»·æ ¼": args.get("price", ""),
        "åŸä»·": "",
        "æƒ³è¦äººæ•°": args.get("wantNum", ""),
        "å–å®¶ID": args.get("seller_id", ""),
        "å–å®¶åç§°": "",
        "å–å®¶å–äº†å¤šå°‘ä»¶å®è´": "",  
        "å–å®¶æ³¨å†Œå¹´é™": "",  
        "å–å®¶æœ€åè®¿é—®æ—¶é—´": "",  
        "å•†å“ä¸»å›¾URLs": "",  
        "å‘å¸ƒæ—¶é—´": convert_timestamp(args.get("publishTime", "")),
        "ä½ç½®": f"{args.get('p_city', '')} {args.get('p_district', '')}".strip(),
        "æ ‡ç­¾": args.get("tagname", ""),
        "æ˜¯å¦è§†é¢‘": "",
        "åˆ†ç±»ID": args.get("catId", ""),
        "æ·˜å®åˆ†ç±»ID": args.get("tbCatId", ""),
        "å­åˆ†ç±»ID": args.get("cCatId", ""),
        "å›¾ç‰‡URL": "",
        "ç”¨æˆ·å¤´åƒ": "",
        "ç”¨æˆ·æ˜µç§°": "",
        "è¯„ä»·æ•°": "",
        "å¥½è¯„ç‡": "",
        "å…¨æ–°æ ‡ç­¾": "",
        "å“ç‰Œæ ‡ç­¾": "",
        "åŒ…é‚®æ ‡ç­¾": "",
        "ç‰¹ä»·æ ‡ç­¾": "",
        "å¥½è¯„æ ‡ç­¾": "",
        "èŠéº»ä¿¡ç”¨": args.get("zhimaOffline", ""),
        "ç›®æ ‡URL": "",
        "è¯¦æƒ…é¡µç±»å‹": "",
        "æ˜¯å¦æ‹å–": ""
    }
    if "detailParams" in ex_content:
        detail_params = ex_content["detailParams"]
        item_info["å•†å“æ ‡é¢˜"] = detail_params.get("title", "")
        item_info["å–å®¶åç§°"] = detail_params.get("userNick", "")
        item_info["æ˜¯å¦è§†é¢‘"] = detail_params.get("isVideo", "")
        item_info["å•†å“ID"] = detail_params.get("itemId", item_info["å•†å“ID"])
    if "area" in ex_content:
        item_info["ä½ç½®"] = ex_content["area"]
    item_info["å›¾ç‰‡URL"] = ex_content.get("picUrl", "")
    if "oriPrice" in ex_content:
        item_info["åŸä»·"] = ex_content["oriPrice"]
    item_info["ç”¨æˆ·å¤´åƒ"] = ex_content.get("userAvatarUrl", "")
    item_info["ç”¨æˆ·æ˜µç§°"] = ex_content.get("userNickName", "")
    item_info["ç›®æ ‡URL"] = main.get("targetUrl", "")
    item_info["è¯¦æƒ…é¡µç±»å‹"] = ex_content.get("detailPageType", "")
    item_info["æ˜¯å¦æ‹å–"] = ex_content.get("isAuction", "")
    if "fishTags" in ex_content:
        fish_tags = ex_content["fishTags"]
        for tag_type in ["r1", "r2", "r3", "r4", "r5"]:
            if tag_type in fish_tags and "tagList" in fish_tags[tag_type]:
                for tag in fish_tags[tag_type]["tagList"]:
                    tag_content = tag.get("data", {}).get("content", "")
                    if tag_type == "r1" and "freeShippingIcon" in tag_content:
                        item_info["åŒ…é‚®æ ‡ç­¾"] = "åŒ…é‚®"
                    elif tag_type == "r2":
                        if "å…¨æ–°" in tag_content:
                            item_info["å…¨æ–°æ ‡ç­¾"] = "å…¨æ–°"
                        elif "ZTT" in tag_content or "ä¸­å¤©" in tag_content:
                            item_info["å“ç‰Œæ ‡ç­¾"] = tag_content
                    elif tag_type == "r3" and "Â¥" in tag_content:
                        item_info["ç‰¹ä»·æ ‡ç­¾"] = tag_content
                    elif tag_type == "r4" and "å¥½è¯„" in tag_content:
                        item_info["å¥½è¯„æ ‡ç­¾"] = tag_content
    if "userFishShopLabel" in ex_content and "tagList" in ex_content["userFishShopLabel"]:
        for tag in ex_content["userFishShopLabel"]["tagList"]:
            tag_content = tag.get("data", {}).get("content", "")
            if "è¯„ä»·" in tag_content:
                item_info["è¯„ä»·æ•°"] = tag_content
            elif "å¥½è¯„ç‡" in tag_content:
                item_info["å¥½è¯„ç‡"] = tag_content
    if not item_info["å•†å“æ ‡é¢˜"]:
        if "title" in item_data:
            item_info["å•†å“æ ‡é¢˜"] = item_data["title"]
        elif "richTitle" in ex_content:
            rich_title = ex_content["richTitle"]
            for title_part in rich_title:
                if title_part.get("type") == "Text" and "data" in title_part:
                    item_info["å•†å“æ ‡é¢˜"] = title_part["data"].get("text", "")
                    break
    item_id = item_info["å•†å“ID"]
    if item_id and item_id in item_details_cache:
        detail_info = item_details_cache[item_id]
        if isinstance(detail_info, dict):
            item_info["å–å®¶å–äº†å¤šå°‘ä»¶å®è´"] = detail_info.get("å–å®¶å–äº†å¤šå°‘ä»¶å®è´", "")
            item_info["å–å®¶æ³¨å†Œå¹´é™"] = detail_info.get("å–å®¶æ³¨å†Œå¹´é™", "")
            item_info["å–å®¶æœ€åè®¿é—®æ—¶é—´"] = detail_info.get("å–å®¶æœ€åè®¿é—®æ—¶é—´", "")
            item_info["å•†å“ä¸»å›¾URLs"] = detail_info.get("å•†å“ä¸»å›¾URLs", "")
        else:
            item_info["å–å®¶å–äº†å¤šå°‘ä»¶å®è´"] = detail_info
    return item_info
def process_detail_api_response(response_data: Dict[str, Any], item_id: str):
    try:
        if "data" in response_data:
            detail_info = {}
            if "sellerDO" in response_data["data"]:
                seller_data = response_data["data"]["sellerDO"]
                detail_info["å–å®¶å–äº†å¤šå°‘ä»¶å®è´"] = seller_data.get("itemCount", "")
                user_reg_day = seller_data.get("userRegDay", 0)
                if user_reg_day:
                    years_from_days = user_reg_day / 365.25
                    detail_info["å–å®¶æ³¨å†Œå¹´é™"] = f"{years_from_days:.1f}å¹´"
                else:
                    detail_info["å–å®¶æ³¨å†Œå¹´é™"] = ""
                detail_info["å–å®¶æœ€åè®¿é—®æ—¶é—´"] = seller_data.get("lastVisitTime", "")
            if "itemDO" in response_data["data"] and "imageInfos" in response_data["data"]["itemDO"]:
                image_infos = response_data["data"]["itemDO"]["imageInfos"]
                image_urls = [img.get("url", "") for img in image_infos if img.get("url")]
                detail_info["å•†å“ä¸»å›¾URLs"] = ",".join(image_urls)
            else:
                detail_info["å•†å“ä¸»å›¾URLs"] = ""
            item_details_cache[item_id] = detail_info
            logger.info(f"âœ… å•†å“ {item_id} è¯¦æƒ…è·å–æˆåŠŸ - å–å®¶å–äº†: {detail_info.get('å–å®¶å–äº†å¤šå°‘ä»¶å®è´', '')}ä»¶å®è´, ä¸»å›¾æ•°: {len(detail_info.get('å•†å“ä¸»å›¾URLs', '').split(',')) if detail_info.get('å•†å“ä¸»å›¾URLs') else 0}")
            return detail_info.get("å–å®¶å–äº†å¤šå°‘ä»¶å®è´", "")
        else:
            logger.warning(f"å•†å“ {item_id} è¯¦æƒ…å“åº”æ•°æ®æ ¼å¼å¼‚å¸¸")
            return ""
    except Exception as e:
        logger.error(f"å¤„ç†å•†å“ {item_id} è¯¦æƒ…æ•°æ®æ—¶å‡ºé”™: {e}")
        return ""
def process_api_response(response_data: Dict[str, Any], page_num: int) -> int:
    items_count = 0
    try:
        if "data" in response_data and "resultList" in response_data["data"]:
            items = response_data["data"]["resultList"]
            for item_data in items:
                if "data" in item_data and "item" in item_data["data"]:
                    item_info = extract_item_info(item_data["data"]["item"])
                    if item_info:
                        item_info["é¡µç "] = page_num
                        all_products_data.append(item_info)
                        items_count += 1
            logger.info(f"ç¬¬ {page_num} é¡µå¤„ç†äº† {items_count} ä¸ªå•†å“")
        else:
            logger.warning(f"ç¬¬ {page_num} é¡µå“åº”æ•°æ®æ ¼å¼å¼‚å¸¸")
    except Exception as e:
        logger.error(f"å¤„ç†ç¬¬ {page_num} é¡µæ•°æ®æ—¶å‡ºé”™: {e}")
    return items_count
def save_products_to_csv(keyword: str):
    if not all_products_data:
        logger.warning("æ²¡æœ‰å•†å“æ•°æ®éœ€è¦ä¿å­˜")
        return
    safe_keyword = "".join(c for c in keyword if c.isalnum() or c in (' ', '-', '_')).rstrip()
    filename = f"{safe_keyword}_å•†å“æ•°æ®.csv"
    try:
        with open(filename, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            headers = [
                "åºå·", "é¡µç ", "å•†å“ID", "å•†å“æ ‡é¢˜", "ä»·æ ¼", "åŸä»·", "æƒ³è¦äººæ•°", "å–å®¶ID", "å–å®¶åç§°", "å–å®¶å–äº†å¤šå°‘ä»¶å®è´",
                "å–å®¶æ³¨å†Œå¹´é™", "å–å®¶æœ€åè®¿é—®æ—¶é—´", "å•†å“ä¸»å›¾URLs",
                "å‘å¸ƒæ—¶é—´", "ä½ç½®", "æ ‡ç­¾", "æ˜¯å¦è§†é¢‘", "åˆ†ç±»ID", "æ·˜å®åˆ†ç±»ID",
                "å­åˆ†ç±»ID", "å›¾ç‰‡URL", "ç”¨æˆ·å¤´åƒ", "ç”¨æˆ·æ˜µç§°", "è¯„ä»·æ•°", "å¥½è¯„ç‡",
                "å…¨æ–°æ ‡ç­¾", "å“ç‰Œæ ‡ç­¾", "åŒ…é‚®æ ‡ç­¾", "ç‰¹ä»·æ ‡ç­¾", "å¥½è¯„æ ‡ç­¾",
                "èŠéº»ä¿¡ç”¨", "ç›®æ ‡URL", "è¯¦æƒ…é¡µç±»å‹", "æ˜¯å¦æ‹å–"
            ]
            writer.writerow(headers)
            for i, item in enumerate(all_products_data, 1):
                item["åºå·"] = i
                row = []
                for header in headers:
                    value = item.get(header, "")
                    if header in ["å•†å“ID", "å–å®¶ID", "åˆ†ç±»ID", "æ·˜å®åˆ†ç±»ID", "å­åˆ†ç±»ID"] and value:
                        value = f"'{value}"
                    row.append(value)
                writer.writerow(row)
        logger.info(f"å•†å“æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        logger.info(f"æ€»å…±ä¿å­˜äº† {len(all_products_data)} ä¸ªå•†å“")
    except Exception as e:
        logger.error(f"ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
def fetch_item_details(page, item_ids: List[str]):
    logger.info(f"å¼€å§‹è·å– {len(item_ids)} ä¸ªå•†å“çš„è¯¦æƒ…ä¿¡æ¯...")
    success_count = 0
    for i, item_id in enumerate(item_ids, 1):
        try:
            if page.is_closed():
                logger.error("é¡µé¢å·²å…³é—­ï¼Œæ— æ³•ç»§ç»­è·å–è¯¦æƒ…")
                break
            detail_url = f"https://www.goofish.com/item?id={item_id}"
            logger.debug(f"è®¿é—®å•†å“è¯¦æƒ…é¡µ ({i}/{len(item_ids)}): {item_id}")
            cache_before = len(item_details_cache)
            page.goto(detail_url, timeout=30000)
            page.wait_for_load_state('networkidle', timeout=10000)
            page.wait_for_timeout(2000)  
            cache_after = len(item_details_cache)
            if item_id in item_details_cache:
                detail_info = item_details_cache[item_id]
                if isinstance(detail_info, dict):
                    sold_count = detail_info.get("å–å®¶å–äº†å¤šå°‘ä»¶å®è´", "")
                    logger.info(f"âœ… å•†å“ {item_id} è¯¦æƒ…è·å–æˆåŠŸ (å–å®¶å–äº†: {sold_count}ä»¶å®è´)")
                else:
                    logger.info(f"âœ… å•†å“ {item_id} è¯¦æƒ…è·å–æˆåŠŸ (å–å®¶å–äº†: {detail_info}ä»¶å®è´)")
                success_count += 1
            elif cache_after > cache_before:
                logger.info(f"âœ… è·å–åˆ°è¯¦æƒ…æ•°æ®ï¼Œä½†å•†å“IDå¯èƒ½ä¸åŒ¹é…")
                success_count += 1
            else:
                logger.warning(f"âš ï¸ å•†å“ {item_id} è¯¦æƒ…è·å–å¤±è´¥")
            if i % 10 == 0:
                logger.info(f"å·²å¤„ç† {i}/{len(item_ids)} ä¸ªå•†å“è¯¦æƒ…ï¼ŒæˆåŠŸ {success_count} ä¸ª")
                page.wait_for_timeout(3000)  
            else:
                page.wait_for_timeout(1000)   
        except Exception as e:
            logger.error(f"è·å–å•†å“ {item_id} è¯¦æƒ…æ—¶å‡ºé”™: {e}")
            if "closed" in str(e).lower():
                logger.error("é¡µé¢å·²å…³é—­ï¼Œåœæ­¢è·å–è¯¦æƒ…")
                break
            continue
    logger.info(f"å•†å“è¯¦æƒ…è·å–å®Œæˆï¼ŒæˆåŠŸè·å– {success_count} ä¸ªå•†å“çš„è¯¦æƒ…")
def load_cookies_from_file():
    try:
        if os.path.exists('cookies.json'):
            with open('cookies.json', 'r', encoding='utf-8') as f:
                cookies_data = json.load(f)
                if isinstance(cookies_data, dict) and 'cookies' in cookies_data:
                    cookies = cookies_data['cookies']
                elif isinstance(cookies_data, list):
                    cookies = cookies_data
                else:
                    logger.error("âŒ cookies.jsonæ ¼å¼ä¸æ­£ç¡®")
                    return []
                logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªcookies")
                return cookies
        else:
            logger.warning("âš ï¸ cookies.jsonæ–‡ä»¶ä¸å­˜åœ¨")
            return []
    except Exception as e:
        logger.error(f"âŒ åŠ è½½cookies.jsonå¤±è´¥: {e}")
        return []
def run(playwright: Playwright) -> None:
    global all_products_data, item_details_cache
    all_products_data = []  
    item_details_cache = {}  
    logger.info("=" * 60)
    logger.info("ğŸŸ é—²é±¼å•†å“è‡ªåŠ¨åŒ–é‡‡é›†å·¥å…·")
    logger.info("=" * 60)
    keyword = os.getenv('DEFAULT_KEYWORD', 'ç§»åŠ¨å…‰ç¼†')
    show_browser = os.getenv('SHOW_BROWSER', 'true').lower() == 'true'
    headless_mode = not show_browser
    logger.info(f"æœç´¢å…³é”®è¯: {keyword}")
    logger.info(f"æµè§ˆå™¨æ¨¡å¼: {'æ˜¾ç¤ºç•Œé¢' if show_browser else 'æ— ç•Œé¢æ¨¡å¼'}")
    browser = playwright.chromium.launch(headless=headless_mode)
    cookies = load_cookies_from_file()
    context = browser.new_context()
    if cookies:
        context.add_cookies(cookies)
        logger.info("âœ… Cookieså·²æ·»åŠ åˆ°æµè§ˆå™¨context")
    page = context.new_page()
    try:
        log_step(1, "è®¿é—®é—²é±¼ç½‘ç«™")
        page.goto("https://www.goofish.com/")
        page.wait_for_load_state('networkidle')
        page.wait_for_timeout(3000)
        logger.info("é¡µé¢åŠ è½½å®Œæˆ")
        log_step(2, f"æœç´¢å•†å“: {keyword}")
        search_selectors = [
            'input[class*="search-input"]',
            'input[type="text"][maxlength="100"]',
            'input[type="text"]',
        ]
        search_box = None
        for selector in search_selectors:
            try:
                search_box = page.locator(selector).first
                if search_box.is_visible(timeout=3000):
                    logger.info(f"æ‰¾åˆ°æœç´¢æ¡†: {selector}")
                    break
            except:
                continue
        if search_box is None:
            logger.error("æ— æ³•æ‰¾åˆ°æœç´¢æ¡†")
            raise Exception("æ— æ³•æ‰¾åˆ°æœç´¢æ¡†")
        search_box.click()
        search_box.clear()
        search_box.fill(keyword)
        logger.info(f"å·²è¾“å…¥å…³é”®è¯: {keyword}")
        try:
            search_button = page.get_by_role("button", name="æœç´¢")
            if search_button.is_visible(timeout=2000):
                search_button.click()
                logger.info("ç‚¹å‡»æœç´¢æŒ‰é’®")
            else:
                search_box.press("Enter")
                logger.info("æŒ‰å›è½¦é”®æœç´¢")
        except:
            search_box.press("Enter")
            logger.info("ä½¿ç”¨å›è½¦é”®æœç´¢")
        page.wait_for_load_state('networkidle')
        logger.info("æœç´¢ç»“æœåŠ è½½å®Œæˆ")
        area_province = os.getenv('AREA_PROVINCE', 'å±±è¥¿')
        area_city = os.getenv('AREA_CITY', 'å…¨å±±è¥¿')
        api_responses = []
        current_page_num = 1
        listen_search_api = True
        def handle_response(response):
            nonlocal current_page_num, listen_search_api
            if listen_search_api and "mtop.taobao.idlemtopsearch.pc.search" in response.url:
                try:
                    if response.status == 200:
                        response_data = response.json()
                        logger.debug(f"æ•è·åˆ°æœç´¢APIå“åº”ï¼Œé¡µç : {current_page_num}")
                        items_count = process_api_response(response_data, current_page_num)
                        if items_count > 0:
                            logger.info(f"ç¬¬ {current_page_num} é¡µè·å–åˆ° {items_count} ä¸ªå•†å“")
                        api_responses.append({
                            'page': current_page_num,
                            'data': response_data,
                            'items_count': items_count
                        })
                    else:
                        logger.warning(f"APIå“åº”çŠ¶æ€ç å¼‚å¸¸: {response.status}")
                except Exception as e:
                    logger.error(f"å¤„ç†APIå“åº”æ—¶å‡ºé”™: {e}")
            elif "mtop.taobao.idle.pc.detail" in response.url:
                try:
                    if response.status == 200:
                        response_data = response.json()
                        item_id = None
                        try:
                            post_data = response.request.post_data or ""
                            if "itemId" in post_data:
                                import urllib.parse
                                parsed_data = urllib.parse.parse_qs(post_data)
                                if "data" in parsed_data:
                                    data_json = json.loads(parsed_data["data"][0])
                                    item_id = data_json.get("itemId")
                        except:
                            import re
                            url_match = re.search(r'"itemId":"(\d+)"', post_data)
                            if url_match:
                                item_id = url_match.group(1)
                        if item_id:
                            logger.debug(f"æ•è·åˆ°å•†å“è¯¦æƒ…APIå“åº”ï¼Œå•†å“ID: {item_id}")
                            process_detail_api_response(response_data, item_id)
                        else:
                            logger.warning("æ— æ³•ä»è¯¦æƒ…APIå“åº”ä¸­æå–å•†å“ID")
                except Exception as e:
                    logger.error(f"å¤„ç†è¯¦æƒ…APIå“åº”æ—¶å‡ºé”™: {e}")
        page.on("response", handle_response)
        log_step(3, f"è®¾ç½®åœ°åŒºç­›é€‰ ({area_province} - {area_city})")
        try:
            area_button = page.locator('div[class*="areaText"]').first
            if area_button.is_visible(timeout=3000):
                logger.info("æ‰¾åˆ°åŒºåŸŸæŒ‰é’®ï¼Œè¿›è¡Œé¼ æ ‡æ‚¬åœ")
                area_button.hover()
                page.wait_for_timeout(1000)
                try:
                    page.get_by_text(area_province).click()
                    page.wait_for_timeout(500)
                    logger.info(f"é€‰æ‹©çœä»½: {area_province}")
                except Exception as e:
                    logger.warning(f"é€‰æ‹©çœä»½ {area_province} å¤±è´¥: {e}")
                    raise
                try:
                    page.get_by_text(area_city).click()
                    page.wait_for_timeout(1000)
                    logger.info(f"é€‰æ‹©åŸå¸‚: {area_city}")
                except Exception as e:
                    logger.warning(f"é€‰æ‹©åŸå¸‚ {area_city} å¤±è´¥: {e}")
                    raise
                try:
                    view_button = page.get_by_text(text=re.compile(r"æŸ¥çœ‹.*ä»¶å®è´"))
                    if view_button.is_visible(timeout=3000):
                        button_text = view_button.text_content()
                        view_button.click()
                        page.wait_for_timeout(1000)
                        logger.info(f"ç‚¹å‡» {button_text}")
                        logger.info("åœ°åŒºç­›é€‰è®¾ç½®å®Œæˆ")
                        page.wait_for_load_state('networkidle')
                        page.wait_for_timeout(2000)  
                    else:
                        logger.warning("æœªæ‰¾åˆ°æŸ¥çœ‹å®è´æŒ‰é’®")
                except Exception as e:
                    logger.warning(f"ç‚¹å‡»æŸ¥çœ‹å®è´æŒ‰é’®å¤±è´¥: {e}")
            else:
                logger.warning("æœªæ‰¾åˆ°åŒºåŸŸæŒ‰é’®ï¼Œè·³è¿‡åœ°åŒºç­›é€‰")
        except Exception as e:
            logger.error(f"åœ°åŒºç­›é€‰å¤±è´¥: {e}")
            logger.warning("è·³è¿‡åœ°åŒºç­›é€‰ï¼Œç»§ç»­æ‰§è¡Œ")
        log_step(4, "å¼€å§‹ç¿»é¡µæµè§ˆ")
        try:
            page_info = page.locator('span[class*="search-page-tiny-page"]').first
            if page_info.is_visible(timeout=3000):
                page_text = page_info.text_content()
                total_pages = int(page_text.split('/')[1]) if '/' in page_text else 1
                logger.info(f"æ£€æµ‹åˆ°æ€»é¡µæ•°: {total_pages}")
            else:
                total_pages = 3
                logger.warning("æœªæ£€æµ‹åˆ°é¡µæ•°ä¿¡æ¯ï¼Œé»˜è®¤ç¿»3é¡µ")
        except:
            total_pages = 3
            logger.warning("é¡µæ•°è§£æå¤±è´¥ï¼Œé»˜è®¤ç¿»3é¡µ")
        max_pages_setting = os.getenv('MAX_PAGES', '5').lower().strip()
        if max_pages_setting == 'max':
            max_pages = total_pages - 1
            logger.info(f"é…ç½®ä¸ºç¿»å®Œæ‰€æœ‰é¡µé¢ (æ€»å…± {total_pages} é¡µï¼Œéœ€ç¿» {max_pages} é¡µ)")
        else:
            try:
                max_pages_config = int(max_pages_setting)
                max_pages = min(total_pages - 1, max_pages_config)
                logger.info(f"é…ç½®ç¿»é¡µ {max_pages} é¡µ (é…ç½®: {max_pages_config}é¡µï¼Œæ€»é¡µæ•°: {total_pages}é¡µ)")
            except ValueError:
                max_pages = min(total_pages - 1, 5)
                logger.warning(f"é…ç½®å€¼ '{max_pages_setting}' æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ç¿»é¡µ {max_pages} é¡µ")
        for i in range(max_pages):
            try:
                current_page_num = i + 2  
                logger.info(f"ğŸ“„ ç¿»åˆ°ç¬¬ {current_page_num} é¡µ")
                top_next_button = page.locator('div[class*="search-page-tiny-arrow-right"]').first
                if top_next_button.is_visible(timeout=2000):
                    logger.debug("ä½¿ç”¨é¡¶éƒ¨ç¿»é¡µæŒ‰é’®")
                    top_next_button.click()
                    page.wait_for_load_state('networkidle')
                    page.wait_for_timeout(2000)
                    logger.info(f"âœ… å·²ç¿»åˆ°ç¬¬ {current_page_num} é¡µ")
                    continue
                logger.debug("é¡¶éƒ¨æŒ‰é’®ä¸å¯ç”¨ï¼Œå°è¯•åº•éƒ¨åˆ†é¡µå™¨")
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(1000)
                bottom_next_button = page.locator('div[class*="search-pagination-arrow-right"]').first
                if bottom_next_button.is_visible(timeout=3000):
                    logger.debug("ä½¿ç”¨åº•éƒ¨ç¿»é¡µæŒ‰é’®")
                    bottom_next_button.click()
                    page.wait_for_load_state('networkidle')
                    page.wait_for_timeout(2000)
                    logger.info(f"âœ… å·²ç¿»åˆ°ç¬¬ {current_page_num} é¡µ")
                    page.evaluate("window.scrollTo(0, 0)")
                    page.wait_for_timeout(1000)
                    continue
                try:
                    page_number_button = page.locator(f'div[class*="search-pagination-page-box"]:has-text("{current_page_num}")').first
                    if page_number_button.is_visible(timeout=2000):
                        logger.debug(f"ç›´æ¥ç‚¹å‡»é¡µç  {current_page_num}")
                        page_number_button.click()
                        page.wait_for_load_state('networkidle')
                        page.wait_for_timeout(2000)
                        logger.info(f"âœ… å·²ç¿»åˆ°ç¬¬ {current_page_num} é¡µ")
                        continue
                except:
                    pass
                logger.error("æ‰€æœ‰ç¿»é¡µæ–¹æ³•éƒ½å¤±è´¥ï¼Œåœæ­¢ç¿»é¡µ")
                break
            except Exception as e:
                logger.error(f"ç¿»é¡µæ—¶å‡ºç°é”™è¯¯: {e}")
                break
        if all_products_data:
            listen_search_api = False
            logger.debug("å·²åœæ­¢ç›‘å¬æœç´¢APIï¼Œä¸“æ³¨äºè¯¦æƒ…API")
            log_step(5, "è·å–å•†å“è¯¦æƒ…ä¿¡æ¯")
            item_ids = []
            for item in all_products_data:
                item_id = item.get("å•†å“ID", "")
                if item_id and item_id not in item_ids:
                    item_ids.append(item_id)
            logger.info(f"éœ€è¦è·å– {len(item_ids)} ä¸ªå•†å“çš„è¯¦æƒ…ä¿¡æ¯")
            try:
                fetch_item_details(page, item_ids)
                for item in all_products_data:
                    item_id = item.get("å•†å“ID", "")
                    if item_id and item_id in item_details_cache:
                        detail_info = item_details_cache[item_id]
                        if isinstance(detail_info, dict):
                            item["å–å®¶å–äº†å¤šå°‘ä»¶å®è´"] = detail_info.get("å–å®¶å–äº†å¤šå°‘ä»¶å®è´", "")
                            item["å–å®¶æ³¨å†Œå¹´é™"] = detail_info.get("å–å®¶æ³¨å†Œå¹´é™", "")
                            item["å–å®¶æœ€åè®¿é—®æ—¶é—´"] = detail_info.get("å–å®¶æœ€åè®¿é—®æ—¶é—´", "")
                            item["å•†å“ä¸»å›¾URLs"] = detail_info.get("å•†å“ä¸»å›¾URLs", "")
                        else:
                            item["å–å®¶å–äº†å¤šå°‘ä»¶å®è´"] = detail_info
                logger.info(f"æˆåŠŸè·å– {len(item_details_cache)} ä¸ªå•†å“çš„è¯¦æƒ…ä¿¡æ¯")
            except Exception as e:
                logger.error(f"è·å–å•†å“è¯¦æƒ…æ—¶å‡ºé”™: {e}")
                logger.warning("è·³è¿‡å•†å“è¯¦æƒ…è·å–ï¼Œç»§ç»­ä¿å­˜åŸºç¡€æ•°æ®")
        logger.info("\n" + "=" * 50)
        logger.info("âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
        logger.info("=" * 50)
        if all_products_data:
            save_products_to_csv(keyword)
        else:
            logger.warning("æ²¡æœ‰é‡‡é›†åˆ°å•†å“æ•°æ®")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    finally:
        logger.info("æ­£åœ¨æ¸…ç†èµ„æº...")
        try:
            context.close()
            browser.close()
        except:
            pass
        logger.info("èµ„æºæ¸…ç†å®Œæˆ")
        
if __name__ == "__main__":
    with sync_playwright() as playwright:
        run(playwright)